{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random, copy\n",
    "import gym\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from matplotlib.image import imsave\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if USE_CUDA else 'cpu')\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "  def __init__(self,capacity): \n",
    "    self.capacity = capacity\n",
    "    self.buffer = []\n",
    "    self.position = 0\n",
    "\n",
    "  def push(self, state, action, reward, next_state, done):\n",
    "      state      = np.expand_dims(state, 0)\n",
    "      next_state = np.expand_dims(next_state, 0)\n",
    "          \n",
    "      self.buffer.append((state, action, reward, next_state, done))\n",
    "  \n",
    "  def sample(self, batch_size):\n",
    "      state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "      return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "  \n",
    "  def __len__(self):\n",
    "      return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticMDP:\n",
    "    def __init__(self):\n",
    "        self.end           = False\n",
    "        self.current_state = 2\n",
    "        self.num_actions   = 2\n",
    "        self.num_states    = 6\n",
    "        self.p_right       = 0.5\n",
    "\n",
    "    def reset(self):\n",
    "        self.end = False\n",
    "        self.current_state = 2\n",
    "        state = np.zeros(self.num_states)\n",
    "        state[self.current_state - 1] = 1.\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.current_state != 1:\n",
    "            if action == 1:\n",
    "                if random.random() < self.p_right and self.current_state < self.num_states:\n",
    "                    self.current_state += 1\n",
    "                else:\n",
    "                    self.current_state -= 1\n",
    "                    \n",
    "            if action == 0:\n",
    "                self.current_state -= 1\n",
    "                \n",
    "            if self.current_state == self.num_states:\n",
    "                self.end = True\n",
    "        \n",
    "        state = np.zeros(self.num_states)\n",
    "        state[self.current_state - 1] = 1.\n",
    "        \n",
    "        if self.current_state == 1:\n",
    "            if self.end:\n",
    "                return state, 1.00, True, {}\n",
    "            else:\n",
    "                return state, 1.00/100.00, True, {}\n",
    "        else:\n",
    "            return state, 0.0, False, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_inputs, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_outputs),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            state  = torch.FloatTensor(state).unsqueeze(0)\n",
    "            action = self.forward(Variable(state, volatile=True)).max(1)[1]\n",
    "            return action.item()\n",
    "        else:\n",
    "            return random.randrange(self.num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(batch_size):\n",
    "    teacher_state, teacher_action, teacher_reward, teacher_next_state, teacher_done = teacher_replay_buffer.sample(batch_size)\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "    \n",
    "    state      = Variable(torch.FloatTensor(state))\n",
    "    next_state = Variable(torch.FloatTensor(next_state), volatile=True)\n",
    "    action     = Variable(torch.LongTensor(action))\n",
    "    reward     = Variable(torch.FloatTensor(reward))\n",
    "    done       = Variable(torch.FloatTensor(done))\n",
    "\n",
    "    teacher_state      = Variable(torch.FloatTensor(teacher_state))\n",
    "    teacher_next_state = Variable(torch.FloatTensor(teacher_next_state), volatile=True)\n",
    "    teacher_action     = Variable(torch.LongTensor(teacher_action))\n",
    "    teacher_reward     = Variable(torch.FloatTensor(teacher_reward))\n",
    "    teacher_done       = Variable(torch.FloatTensor(teacher_done))\n",
    "    \n",
    "    #Model update\n",
    "    q_value = model(state)\n",
    "    q_value = q_value.gather(1, action.unsqueeze(1)).squeeze(1)    \n",
    "    teacher_q_value = teacher_model(teacher_state)\n",
    "    teacher_q_value = teacher_q_value.gather(1, teacher_action.unsqueeze(1)).squeeze(1)\n",
    "    expected_q_value = reward + 0.99 * teacher_q_value * (1 - done)\n",
    "#     loss = (reward - teacher_prim_reward).pow(2).mean()\n",
    "    loss = (q_value - Variable(expected_q_value.data)).pow(2).mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #Teacher update\n",
    "    next_q_value     = teacher_model(teacher_next_state).max(1)[0]\n",
    "    expected_q_value = teacher_reward + 0.99 * next_q_value * (1 - teacher_done)\n",
    "    teacher_loss = (teacher_q_value - Variable(expected_q_value.data)).pow(2).mean()\n",
    "    teacher_optimizer.zero_grad()\n",
    "    teacher_loss.backward()\n",
    "    teacher_optimizer.step()\n",
    "    \n",
    "    return loss,teacher_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')#StochasticMDP()\n",
    "learning_rate = 0.001\n",
    "model        = Net(env.observation_space.shape[0]+2, env.action_space.n).to(device)\n",
    "teacher_model        = Net(env.observation_space.shape[0]+2, env.action_space.n).to(device)\n",
    "optimizer      = optim.Adam(model.parameters(),lr=learning_rate)\n",
    "teacher_optimizer      = optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "replay_buffer      = ReplayBuffer(1000000)\n",
    "teacher_replay_buffer      = ReplayBuffer(1000000)\n",
    "replay_initial = 100\n",
    "\n",
    "load_model = False\n",
    "if load_model==True:\n",
    "    model_checkpoint = torch.load('./Checkpoint/model.pth.tar', map_location=device) \n",
    "    model.load_state_dict(model_checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(model_checkpoint['optimizer_state_dict'])\n",
    "    loss = model_checkpoint['loss']\n",
    "\n",
    "load_teacher_model = False\n",
    "if load_teacher_model==True:\n",
    "    teacher_model_checkpoint = torch.load('./Checkpoint/teacher_model.pth.tar', map_location=device) \n",
    "    teacher_model.load_state_dict(teacher_model_checkpoint['model_state_dict'])\n",
    "    teacher_optimizer.load_state_dict(teacher_model_checkpoint['optimizer_state_dict'])\n",
    "    teacher_loss = model_checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14f7d314748>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 10000\n",
    "\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)\n",
    "plt.figure()\n",
    "plt.plot([epsilon_by_frame(i) for i in range(500000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONLY FOR ATARI\n",
    "action = 1\n",
    "def process_atari(state):\n",
    "    action = 1\n",
    "    for _ in range(3):\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = np.concatenate((state,next_state),0)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K:\\Users\\Karush Suri\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step- 1000 / 500000 |Episode Reward- 11.4 |Loss- 1.1801228523254395\n",
      "Teacher Episode Reward- 10.0 Teacher Loss- 0.910210371017456\n",
      "Step- 2000 / 500000 |Episode Reward- 26.400000000000006 |Loss- 1.1229420900344849\n",
      "Teacher Episode Reward- 24.0 Teacher Loss- 0.941811203956604\n",
      "Step- 3000 / 500000 |Episode Reward- 34.80000000000001 |Loss- 1.032637357711792\n",
      "Teacher Episode Reward- 32.0 Teacher Loss- 0.9308387637138367\n",
      "Step- 4000 / 500000 |Episode Reward- 27.300000000000008 |Loss- 1.114243507385254\n",
      "Teacher Episode Reward- 25.0 Teacher Loss- 0.8486908674240112\n",
      "Step- 5000 / 500000 |Episode Reward- 56.90000000000003 |Loss- 0.9448508620262146\n",
      "Teacher Episode Reward- 53.0 Teacher Loss- 0.958681046962738\n",
      "Step- 6000 / 500000 |Episode Reward- 48.30000000000001 |Loss- 1.0295122861862183\n",
      "Teacher Episode Reward- 45.0 Teacher Loss- 0.9134373664855957\n",
      "Step- 7000 / 500000 |Episode Reward- 13.7 |Loss- 0.92139732837677\n",
      "Teacher Episode Reward- 12.0 Teacher Loss- 0.9579387307167053\n",
      "Step- 8000 / 500000 |Episode Reward- 45.40000000000002 |Loss- 0.9431639909744263\n",
      "Teacher Episode Reward- 42.0 Teacher Loss- 0.9885854125022888\n",
      "Step- 9000 / 500000 |Episode Reward- 38.100000000000016 |Loss- 0.8699752688407898\n",
      "Teacher Episode Reward- 35.0 Teacher Loss- 1.0204411745071411\n",
      "Step- 10000 / 500000 |Episode Reward- 24.400000000000006 |Loss- 0.9313645958900452\n",
      "Teacher Episode Reward- 22.0 Teacher Loss- 0.9829511046409607\n",
      "Step- 11000 / 500000 |Episode Reward- 50.90000000000002 |Loss- 0.9953224658966064\n",
      "Teacher Episode Reward- 47.0 Teacher Loss- 0.951270341873169\n",
      "Step- 12000 / 500000 |Episode Reward- 44.700000000000024 |Loss- 0.9037405252456665\n",
      "Teacher Episode Reward- 41.0 Teacher Loss- 0.996437132358551\n",
      "Step- 13000 / 500000 |Episode Reward- 47.800000000000026 |Loss- 0.734951376914978\n",
      "Teacher Episode Reward- 44.0 Teacher Loss- 0.9727943539619446\n",
      "Step- 14000 / 500000 |Episode Reward- 68.00000000000003 |Loss- 0.9861044883728027\n",
      "Teacher Episode Reward- 63.0 Teacher Loss- 1.027705192565918\n",
      "Step- 15000 / 500000 |Episode Reward- 66.60000000000002 |Loss- 0.7588536739349365\n",
      "Teacher Episode Reward- 62.0 Teacher Loss- 0.9801802635192871\n",
      "Step- 16000 / 500000 |Episode Reward- 42.50000000000002 |Loss- 0.8205184936523438\n",
      "Teacher Episode Reward- 39.0 Teacher Loss- 0.9470940828323364\n",
      "Step- 17000 / 500000 |Episode Reward- 17.900000000000002 |Loss- 0.8804560899734497\n",
      "Teacher Episode Reward- 16.0 Teacher Loss- 0.974173367023468\n",
      "Step- 18000 / 500000 |Episode Reward- 99.99999999999993 |Loss- 0.776988685131073\n",
      "Teacher Episode Reward- 93.0 Teacher Loss- 0.9913880228996277\n",
      "Step- 19000 / 500000 |Episode Reward- 46.60000000000002 |Loss- 0.7498548626899719\n",
      "Teacher Episode Reward- 43.0 Teacher Loss- 0.9969900846481323\n",
      "Step- 20000 / 500000 |Episode Reward- 101.1999999999999 |Loss- 0.751915454864502\n",
      "Teacher Episode Reward- 95.0 Teacher Loss- 0.9834441542625427\n",
      "Step- 21000 / 500000 |Episode Reward- 48.700000000000024 |Loss- 0.6697454452514648\n",
      "Teacher Episode Reward- 45.0 Teacher Loss- 0.9396126866340637\n",
      "Step- 22000 / 500000 |Episode Reward- 20.1 |Loss- 0.6293029189109802\n",
      "Teacher Episode Reward- 18.0 Teacher Loss- 0.9736225008964539\n",
      "Step- 23000 / 500000 |Episode Reward- 78.3 |Loss- 1.0147091150283813\n",
      "Teacher Episode Reward- 73.0 Teacher Loss- 0.967399001121521\n",
      "Step- 24000 / 500000 |Episode Reward- 71.9 |Loss- 0.6982730031013489\n",
      "Teacher Episode Reward- 67.0 Teacher Loss- 0.9948927164077759\n",
      "Step- 25000 / 500000 |Episode Reward- 51.800000000000026 |Loss- 1.0184228420257568\n",
      "Teacher Episode Reward- 48.0 Teacher Loss- 0.969429075717926\n",
      "Step- 26000 / 500000 |Episode Reward- 49.40000000000002 |Loss- 0.8594962358474731\n",
      "Teacher Episode Reward- 46.0 Teacher Loss- 0.9724286198616028\n",
      "Step- 27000 / 500000 |Episode Reward- 43.60000000000002 |Loss- 0.6169566512107849\n",
      "Teacher Episode Reward- 40.0 Teacher Loss- 0.9977574944496155\n",
      "Step- 28000 / 500000 |Episode Reward- 44.60000000000002 |Loss- 0.5757718682289124\n",
      "Teacher Episode Reward- 41.0 Teacher Loss- 0.9810386300086975\n",
      "Step- 29000 / 500000 |Episode Reward- 47.90000000000002 |Loss- 0.37905892729759216\n",
      "Teacher Episode Reward- 44.0 Teacher Loss- 1.0351386070251465\n",
      "Step- 30000 / 500000 |Episode Reward- 82.39999999999998 |Loss- 0.7632286548614502\n",
      "Teacher Episode Reward- 77.0 Teacher Loss- 0.9840003252029419\n",
      "Step- 31000 / 500000 |Episode Reward- 47.50000000000002 |Loss- 0.6417127847671509\n",
      "Teacher Episode Reward- 44.0 Teacher Loss- 1.010480284690857\n",
      "Step- 32000 / 500000 |Episode Reward- 42.50000000000002 |Loss- 0.6875321865081787\n",
      "Teacher Episode Reward- 39.0 Teacher Loss- 0.9904327392578125\n",
      "Step- 33000 / 500000 |Episode Reward- 63.400000000000034 |Loss- 0.524885356426239\n",
      "Teacher Episode Reward- 59.0 Teacher Loss- 0.9475678205490112\n",
      "Step- 34000 / 500000 |Episode Reward- 73.80000000000001 |Loss- 0.6391627788543701\n",
      "Teacher Episode Reward- 69.0 Teacher Loss- 0.9690394401550293\n",
      "Step- 35000 / 500000 |Episode Reward- 60.30000000000003 |Loss- 0.5571497082710266\n",
      "Teacher Episode Reward- 56.0 Teacher Loss- 0.965475857257843\n",
      "Step- 36000 / 500000 |Episode Reward- 57.10000000000003 |Loss- 0.7090888023376465\n",
      "Teacher Episode Reward- 53.0 Teacher Loss- 1.0084511041641235\n",
      "Step- 37000 / 500000 |Episode Reward- 48.60000000000002 |Loss- 0.5032529830932617\n",
      "Teacher Episode Reward- 45.0 Teacher Loss- 0.9369717836380005\n",
      "Step- 38000 / 500000 |Episode Reward- 52.800000000000026 |Loss- 0.4540058970451355\n",
      "Teacher Episode Reward- 49.0 Teacher Loss- 0.9354231357574463\n",
      "Step- 39000 / 500000 |Episode Reward- 48.60000000000002 |Loss- 0.7953169941902161\n",
      "Teacher Episode Reward- 45.0 Teacher Loss- 1.0147104263305664\n",
      "Step- 40000 / 500000 |Episode Reward- 20.1 |Loss- 0.4805035889148712\n",
      "Teacher Episode Reward- 18.0 Teacher Loss- 0.9600970149040222\n",
      "Step- 41000 / 500000 |Episode Reward- 89.79999999999994 |Loss- 0.5703556537628174\n",
      "Teacher Episode Reward- 84.0 Teacher Loss- 0.9821672439575195\n",
      "Step- 42000 / 500000 |Episode Reward- 89.59999999999995 |Loss- 0.45397496223449707\n",
      "Teacher Episode Reward- 84.0 Teacher Loss- 0.9580038189888\n",
      "Step- 43000 / 500000 |Episode Reward- 64.00000000000003 |Loss- 0.6817656755447388\n",
      "Teacher Episode Reward- 60.0 Teacher Loss- 1.007792592048645\n",
      "Step- 44000 / 500000 |Episode Reward- 76.69999999999999 |Loss- 0.5659366250038147\n",
      "Teacher Episode Reward- 72.0 Teacher Loss- 0.9816521406173706\n",
      "Step- 45000 / 500000 |Episode Reward- 74.89999999999999 |Loss- 0.7046710252761841\n",
      "Teacher Episode Reward- 70.0 Teacher Loss- 0.9334566593170166\n",
      "Step- 46000 / 500000 |Episode Reward- 75.0 |Loss- 0.47753340005874634\n",
      "Teacher Episode Reward- 70.0 Teacher Loss- 0.9466543197631836\n",
      "Step- 47000 / 500000 |Episode Reward- 52.800000000000026 |Loss- 0.4852934181690216\n",
      "Teacher Episode Reward- 49.0 Teacher Loss- 0.9573027491569519\n",
      "Step- 48000 / 500000 |Episode Reward- 64.40000000000003 |Loss- 0.4833507835865021\n",
      "Teacher Episode Reward- 60.0 Teacher Loss- 0.9867483377456665\n",
      "Step- 49000 / 500000 |Episode Reward- 77.1 |Loss- 0.46510764956474304\n",
      "Teacher Episode Reward- 72.0 Teacher Loss- 0.9572948813438416\n",
      "Step- 50000 / 500000 |Episode Reward- 102.29999999999991 |Loss- 0.7804566621780396\n",
      "Teacher Episode Reward- 96.0 Teacher Loss- 0.9850579500198364\n",
      "Step- 51000 / 500000 |Episode Reward- 47.30000000000002 |Loss- 0.4063871204853058\n",
      "Teacher Episode Reward- 44.0 Teacher Loss- 0.9597422480583191\n",
      "Step- 52000 / 500000 |Episode Reward- 85.09999999999997 |Loss- 0.6264531016349792\n",
      "Teacher Episode Reward- 80.0 Teacher Loss- 0.9601638913154602\n",
      "Step- 53000 / 500000 |Episode Reward- 71.80000000000001 |Loss- 0.6597211360931396\n",
      "Teacher Episode Reward- 67.0 Teacher Loss- 1.0089359283447266\n",
      "Step- 54000 / 500000 |Episode Reward- 68.70000000000002 |Loss- 0.43779829144477844\n",
      "Teacher Episode Reward- 64.0 Teacher Loss- 0.9096873998641968\n",
      "Step- 55000 / 500000 |Episode Reward- 67.50000000000003 |Loss- 0.41801872849464417\n",
      "Teacher Episode Reward- 63.0 Teacher Loss- 0.9436161518096924\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-34ab23e714e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mreplay_initial\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mteacher_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-c9db1bbda962>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(batch_size)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mq_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mq_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mteacher_q_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mteacher_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mteacher_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mteacher_q_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mteacher_q_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mteacher_action\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mexpected_q_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.99\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mteacher_q_value\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mK:\\Users\\Karush Suri\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-a01fb58e1158>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mK:\\Users\\Karush Suri\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mK:\\Users\\Karush Suri\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mK:\\Users\\Karush Suri\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mK:\\Users\\Karush Suri\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mK:\\Users\\Karush Suri\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mthreshold\u001b[1;34m(input, threshold, value, inplace)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthreshold_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_frames = 10000\n",
    "batch_size = 32\n",
    "gamma      = 0.99\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "avg_rewards = []\n",
    "episode_reward = 0\n",
    "avg_reward = 0\n",
    "\n",
    "teacher_losses = []\n",
    "teacher_all_rewards = []\n",
    "teacher_avg_rewards = []\n",
    "teacher_episode_reward = 0\n",
    "teacher_avg_reward = 0\n",
    "\n",
    "state_rewards = []\n",
    "teacher_reward = 0\n",
    "teacher_action = 1\n",
    "next_state = env.reset()\n",
    "# state = process_atari(state)\n",
    "print(next_state.shape)\n",
    "for frame_idx in range(1, num_frames + 1):\n",
    "#     env_teacher = copy.deepcopy(env)\n",
    "    epsilon = epsilon_by_frame(frame_idx)\n",
    "    state = np.concatenate((next_state,np.array([teacher_action]),np.array([teacher_reward])),0)\n",
    "    \n",
    "    action = model.act(state, epsilon)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state_rewards.append(reward)\n",
    "    if (action==teacher_action):\n",
    "        reward += 0.1*abs(reward)\n",
    "    \n",
    "    env_teacher = copy.deepcopy(env)\n",
    "    teacher_state = np.concatenate((next_state,np.array([action]),np.array([reward])),0)\n",
    "    teacher_action = teacher_model.act(state, epsilon)\n",
    "    teacher_next_state, teacher_reward, teacher_done, _ = env_teacher.step(teacher_action)\n",
    "#     if (reward>state_rewards[-1]):\n",
    "#         teacher_reward += 0.2*teacher_reward\n",
    "    \n",
    "    #Auxilary step for teacher's next-state update\n",
    "    aux_state = np.concatenate((teacher_next_state,np.array([teacher_action]),np.array([teacher_reward])),0)\n",
    "    aux_action = model.act(aux_state, epsilon)\n",
    "    aux_next_state, aux_reward, aux_done, _ = env_teacher.step(aux_action)\n",
    "    teacher_next_state = np.concatenate((aux_next_state,np.array([aux_action]),np.array([aux_reward])),0)\n",
    "    \n",
    "    teacher_replay_buffer.push(teacher_state, teacher_action, teacher_reward, teacher_next_state, teacher_done)\n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "#     state = np.concatenate((state[1:,:,:],next_state),0)\n",
    "    episode_reward += reward\n",
    "    teacher_episode_reward += teacher_reward\n",
    "    \n",
    "    # if frame_idx in range(15000,20000):\n",
    "    #   imsave(checkpoint_name+'/Frames/'+str(frame_idx)+'.png',next_state[0,:,:])\n",
    "\n",
    "    if done:\n",
    "        next_state = env.reset()\n",
    "#         state = process_atari(state)\n",
    "        all_rewards.append(episode_reward)\n",
    "        avg_rewards.append(avg_reward)\n",
    "        episode_reward = 0\n",
    "        teacher_all_rewards.append(teacher_episode_reward)\n",
    "        teacher_avg_rewards.append(teacher_avg_reward)\n",
    "        teacher_episode_reward = 0\n",
    "        teacher_reward = 0\n",
    "        teacher_action = 1\n",
    "\n",
    "        \n",
    "    if len(replay_buffer) > batch_size:\n",
    "        loss,teacher_loss = update(batch_size)\n",
    "        loss = loss.item()\n",
    "        losses.append(loss)\n",
    "        teacher_loss = teacher_loss.item()\n",
    "        teacher_losses.append(teacher_loss)\n",
    "                \n",
    "    if frame_idx % 1000 == 0:\n",
    "        avg_reward = np.mean(all_rewards[-1000:])\n",
    "        teacher_avg_reward = np.mean(teacher_all_rewards[-1000:])\n",
    "        print('Step-',frame_idx,'/',num_frames,'|Episode Reward-',all_rewards[-1],'|Loss-',loss)\n",
    "        print('Teacher Episode Reward-',teacher_all_rewards[-1],'Teacher Loss-',teacher_loss)\n",
    "        torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'loss': loss},\n",
    "                   './Checkpoint/model.pth.tar')\n",
    "        torch.save({'model_state_dict': teacher_model.state_dict(), 'optimizer_state_dict': teacher_optimizer.state_dict(),\n",
    "                    'loss': teacher_loss},'./Checkpoint/teacher_model.pth.tar')\n",
    "        # plt.plot(all_rewards)\n",
    "\n",
    "data_save = {}\n",
    "data_save['loss'] = losses\n",
    "data_save['reward'] = all_rewards\n",
    "data_save['avg_reward'] = avg_rewards\n",
    "\n",
    "teacher_data_save = {}\n",
    "teacher_data_save['loss'] = teacher_losses\n",
    "teacher_data_save['reward'] = teacher_all_rewards\n",
    "teacher_data_save['avg_reward'] = teacher_avg_rewards\n",
    "\n",
    "with open('./Checkpoint/data_save.pkl', 'wb') as f: #data+same as frame folder\n",
    "    pkl.dump(data_save, f)\n",
    "\n",
    "with open('./Checkpoint/teacher_data_save.pkl', 'wb') as f: #data+same as frame folder\n",
    "    pkl.dump(teacher_data_save, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for Model\n",
    "plt.figure()\n",
    "plt.plot(avg_rewards)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for Teacher\n",
    "plt.figure()\n",
    "plt.plot(teacher_avg_rewards)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(teacher_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
