{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ES_atari.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1fOCbFmi08ua"
      },
      "source": [
        "<h1><center> Playing Atari using Evolution Strategies </center></h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y8hU4S5308ue"
      },
      "source": [
        "## 1. Import Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "buWMBu3PhvqU",
        "outputId": "d660fc24-841b-4c92-b472-2d063840ad1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "from google.colab import drive\n",
        "user_name = '/content/drive'\n",
        "drive.mount(user_name, force_remount=True)\n",
        "# !lscpu\n",
        "from __future__ import absolute_import, division, print_function\n",
        "# !pip install torch\n",
        "!pip install gym\n",
        "!pip install gym[atari]\n",
        "import math, random\n",
        "import time\n",
        "import argparse\n",
        "import copy\n",
        "from multiprocessing import Pool\n",
        "# from multiprocessing.pool import ThreadPool\n",
        "from functools import partial\n",
        "import gym\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "from matplotlib.image import imsave\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.autograd as autograd \n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "checkpoint_name = '/content/drive/My Drive/Colab Notebooks/Checkpoint/'\n",
        "\n",
        "USE_CUDA = False\n",
        "device = torch.device('cuda' if USE_CUDA else 'cpu')\n",
        "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.2)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.17.1)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.18.2)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.4.1)\n",
            "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.2.6)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (7.0.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "62A_iOxVVa3o"
      },
      "source": [
        "## 2. Evolution Strategies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ICs9bZPDVOym",
        "colab": {}
      },
      "source": [
        "def get_reward(weights, model, render=False):\n",
        "    cloned_model = copy.deepcopy(model)\n",
        "    for i, param in enumerate(cloned_model.parameters()):\n",
        "        try:\n",
        "            param.data.copy_(weights[i])\n",
        "        except:\n",
        "            param.data.copy_(weights[i].data)\n",
        "\n",
        "    ob, env = process_atari(env_id)\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        batch = torch.FloatTensor(ob[np.newaxis,...]).to(device)\n",
        "        prediction = cloned_model(Variable(batch))\n",
        "        action = prediction.cpu().data.numpy().argmax()\n",
        "        new_ob, reward, done, _ = env.step(action)\n",
        "        new_ob = env.unwrapped._get_ram()\n",
        "        ob = new_ob\n",
        "\n",
        "        total_reward += reward \n",
        "\n",
        "    env.close()\n",
        "    return total_reward\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Linear') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "\n",
        "class EvolutionModule:\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        weights, \n",
        "        reward_func,\n",
        "        population_size=50,\n",
        "        sigma=0.1,\n",
        "        learning_rate=0.001,\n",
        "        decay=1.0,\n",
        "        sigma_decay=1.0,\n",
        "        threadcount=4,\n",
        "        render_test=False,\n",
        "        cuda=False,\n",
        "        reward_goal=None,\n",
        "        consecutive_goal_stopping=None,\n",
        "        save_path=None\n",
        "    ):\n",
        "        np.random.seed(int(time.time()))\n",
        "        self.weights = weights\n",
        "        self.reward_function = reward_func\n",
        "        self.POPULATION_SIZE = population_size\n",
        "        self.SIGMA = sigma\n",
        "        self.LEARNING_RATE = learning_rate\n",
        "        self.cuda = cuda\n",
        "        self.decay = decay\n",
        "        self.sigma_decay = sigma_decay\n",
        "        self.pool = Pool(threadcount) #Thread\n",
        "        self.render_test = render_test\n",
        "        self.reward_goal = reward_goal\n",
        "        self.consecutive_goal_stopping = consecutive_goal_stopping\n",
        "        self.consecutive_goal_count = 0\n",
        "        self.save_path = save_path\n",
        "        self.max_rewards = []\n",
        "        self.min_rewards = []\n",
        "        self.avg_rewards = []\n",
        "\n",
        "\n",
        "    def jitter_weights(self, weights, population=[], no_jitter=False):\n",
        "        new_weights = []\n",
        "        for i, param in enumerate(weights):\n",
        "            if no_jitter:\n",
        "                new_weights.append(param.to(device).detach()) #.data\n",
        "            else:\n",
        "                jittered = torch.FloatTensor(self.SIGMA * population[i]).detach()\n",
        "                jittered = jittered.to(device).detach()\n",
        "                new_weights.append(param.to(device).detach() + jittered) #.data\n",
        "        return new_weights\n",
        "\n",
        "\n",
        "    def run(self, iterations, print_step=100):\n",
        "        for iteration in range(iterations):\n",
        "            torch.cuda.empty_cache()\n",
        "            population = []\n",
        "            for _ in range(self.POPULATION_SIZE):\n",
        "                x = []\n",
        "                for param in self.weights:\n",
        "                    x.append(np.random.randn(*param.data.size()))\n",
        "                population.append(x)\n",
        "\n",
        "            rewards = self.pool.map(\n",
        "                self.reward_function, \n",
        "                [self.jitter_weights(copy.deepcopy(self.weights), population=pop) for pop in population]\n",
        "            )\n",
        "            self.max_rewards.append(max(rewards))\n",
        "            self.min_rewards.append(min(rewards))\n",
        "            self.avg_rewards.append(np.mean(rewards))\n",
        "            if np.std(rewards) != 0:\n",
        "                normalized_rewards = (rewards - np.mean(rewards)) / np.std(rewards)\n",
        "                for index, param in enumerate(self.weights):\n",
        "                    A = np.array([p[index] for p in population])\n",
        "                    rewards_pop = torch.FloatTensor(np.dot(A.T, normalized_rewards).T).to(device)\n",
        "                    param = param.to(device)\n",
        "                    param = param + self.LEARNING_RATE / (self.POPULATION_SIZE * self.SIGMA) * rewards_pop\n",
        "\n",
        "                    self.LEARNING_RATE *= self.decay\n",
        "                    self.SIGMA *= self.sigma_decay\n",
        "\n",
        "            test_reward = self.reward_function(\n",
        "                self.jitter_weights(copy.deepcopy(self.weights), no_jitter=True), render=self.render_test\n",
        "            )\n",
        "            if (iteration+1) % print_step == 0:\n",
        "                print('iter %d. reward: %f' % (iteration+1, test_reward))\n",
        "                if self.save_path:\n",
        "                    pickle.dump(self.weights, open(self.save_path+'weights.pkl', 'wb'))\n",
        "                \n",
        "                if self.reward_goal and self.consecutive_goal_stopping:\n",
        "                    if test_reward >= self.reward_goal:\n",
        "                        self.consecutive_goal_count += 1\n",
        "                    else:\n",
        "                        self.consecutive_goal_count = 0\n",
        "\n",
        "                    if self.consecutive_goal_count >= self.consecutive_goal_stopping:\n",
        "                        return self.weights\n",
        "        return self.weights, self.max_rewards,self.min_rewards,self.avg_rewards"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s3hQs5QYMxGD"
      },
      "source": [
        "## 3. ConvNet Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pwWh3vRWM0PJ",
        "colab": {}
      },
      "source": [
        "class CnnDQN(nn.Module):\n",
        "    def __init__(self, input_shape, num_actions):\n",
        "        super(CnnDQN, self).__init__()\n",
        "        \n",
        "        self.input_shape = input_shape\n",
        "        self.num_actions = num_actions\n",
        "        \n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.feature_size(), 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, self.num_actions)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x   = Variable(torch.FloatTensor(x).unsqueeze(0), volatile=True)\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "    \n",
        "    def feature_size(self):\n",
        "        return self.features(autograd.Variable(torch.zeros(1, *self.input_shape))).view(1, -1).size(1)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_4nnTYob08vC"
      },
      "source": [
        "## 4. Initialize Learning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7TEINiFzlEpb",
        "colab": {}
      },
      "source": [
        "def process_atari(env_id, atari=False):\n",
        "    if atari:\n",
        "      env    = make_atari(env_id)\n",
        "      env    = wrap_deepmind(env)\n",
        "      env    = wrap_pytorch(env)\n",
        "      state = env.reset()\n",
        "      action = 1\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "      for _ in range(3):\n",
        "          state = np.concatenate((state,next_state),0)\n",
        "    else:\n",
        "      env = gym.make(env_id)\n",
        "      state = env.reset()\n",
        "      state = env.unwrapped._get_ram()\n",
        "    return state, env\n",
        "\n",
        "\n",
        "sigma = 1\n",
        "es_learning_rate = 0.0001\n",
        "es_decay = 0.9999\n",
        "threads = 2\n",
        "goal = 300\n",
        "stop = 10\n",
        "es_steps = 1000\n",
        "\n",
        "hyper_epochs = 50\n",
        "pop_size = 2\n",
        "batch_size = 32\n",
        "gamma      = 0.99\n",
        "epsilon = 0.10\n",
        "replay_initial = 32\n",
        "wta_steps = 5000\n",
        "losses = []\n",
        "rewards = []\n",
        "\n",
        "env_id = \"Breakout-v0\"\n",
        "state, env = process_atari(env_id)\n",
        "\n",
        "# model = CnnDQN((4,84,84), env.action_space.n).to(device)\n",
        "\n",
        "# os.sched_setaffinity(os.getpid(), {0})\n",
        "# os.system(\"taskset -p 0xffffffff %d\" % os.getpid())\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(128, 32),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(32,16),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(16,8),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(8, env.action_space.n),\n",
        ").to(device)    \n",
        "\n",
        "# os.sched_setaffinity(os.getpid(), {0})\n",
        "\n",
        "# print(os.sched_getaffinity(os.getpid()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDvGeFVHa433",
        "colab_type": "text"
      },
      "source": [
        "## 5. Learning Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9gBzaUNatHP",
        "colab_type": "code",
        "outputId": "9be03111-b1b7-41f4-a808-ff6c9c16e32a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "partial_func = partial(get_reward, model=model)\n",
        "mother_parameters = list(model.parameters())\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "es = EvolutionModule(\n",
        "    mother_parameters, partial_func, population_size=pop_size, sigma=sigma, \n",
        "    learning_rate=es_learning_rate, decay = es_decay, threadcount=threads, cuda=True, reward_goal=goal,\n",
        "    consecutive_goal_stopping=stop)\n",
        "final_weights,max_rewards,min_rewards,avg_rewards = es.run(es_steps, print_step=10)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "data_save = {}\n",
        "data_save['time'] = [start_time,end_time]\n",
        "data_save['max_rewards'] = max_rewards\n",
        "data_save['min_reward'] = min_rewards\n",
        "data_save['avg_reward'] = avg_rewards\n",
        "\n",
        "with open(checkpoint_name+'data.pkl', 'wb') as f:\n",
        "    pkl.dump(data_save, f)\n",
        "\n",
        "with open(checkpoint_name+'weights.pkl', 'wb') as f:\n",
        "    pkl.dump(final_weights, f)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 10. reward: 0.000000\n",
            "iter 20. reward: 0.000000\n",
            "iter 30. reward: 0.000000\n",
            "iter 40. reward: 0.000000\n",
            "iter 50. reward: 0.000000\n",
            "iter 60. reward: 0.000000\n",
            "iter 70. reward: 0.000000\n",
            "iter 80. reward: 0.000000\n",
            "iter 90. reward: 0.000000\n",
            "iter 100. reward: 0.000000\n",
            "iter 110. reward: 0.000000\n",
            "iter 120. reward: 0.000000\n",
            "iter 130. reward: 0.000000\n",
            "iter 140. reward: 0.000000\n",
            "iter 150. reward: 0.000000\n",
            "iter 160. reward: 0.000000\n",
            "iter 170. reward: 0.000000\n",
            "iter 180. reward: 0.000000\n",
            "iter 190. reward: 0.000000\n",
            "iter 200. reward: 0.000000\n",
            "iter 210. reward: 0.000000\n",
            "iter 220. reward: 0.000000\n",
            "iter 230. reward: 0.000000\n",
            "iter 240. reward: 0.000000\n",
            "iter 250. reward: 0.000000\n",
            "iter 260. reward: 0.000000\n",
            "iter 270. reward: 0.000000\n",
            "iter 280. reward: 0.000000\n",
            "iter 290. reward: 0.000000\n",
            "iter 300. reward: 0.000000\n",
            "iter 310. reward: 0.000000\n",
            "iter 320. reward: 0.000000\n",
            "iter 330. reward: 0.000000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process ForkPoolWorker-24:\n",
            "Process ForkPoolWorker-23:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-6f845005d7e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mes_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mes_decay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreadcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_goal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgoal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     consecutive_goal_stopping=stop)\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mfinal_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_rewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_rewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mavg_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mes_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-2929c47a2b8c>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, iterations, print_step)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             test_reward = self.reward_function(\n\u001b[0;32m--> 109\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjitter_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_jitter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             )\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprint_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-2929c47a2b8c>\u001b[0m in \u001b[0;36mget_reward\u001b[0;34m(weights, model, render)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcloned_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mnew_ob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mnew_ob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_ob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/atari_py/ale_python_interface.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4BzNaawTIrul"
      },
      "source": [
        "## 6. Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nqv_b9Ol7bMK",
        "outputId": "a7e74f18-8fdb-494e-ebdd-a27573d4a877",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "sns.set_style('darkgrid')\n",
        "plt.plot(avg_rewards,color='royalblue')\n",
        "plt.fill_between(np.arange(0,len(avg_rewards),1),min_rewards,max_rewards,facecolor='lightsteelblue',linewidth=0)\n",
        "\n",
        "print(end_time - start_time)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-165.17953300476074\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NTIDQpRIIvfl"
      },
      "source": [
        "## 5. References\n",
        "\n",
        "[Karpathy's Blog](https://gist.github.com/karpathy/77fbb6a8dac5395f1b73e7a89300318d)\n",
        "\n",
        "[Evolution directed towards Exploration](https://papers.nips.cc/paper/7750-improving-exploration-in-evolution-strategies-for-deep-reinforcement-learning-via-a-population-of-novelty-seeking-agents.pdf)\n",
        "\n",
        "[A more readable version of the code in PyTorch](https://github.com/staturecrane/PyTorch-ES)\n",
        "\n",
        "[Dreamer (This paper came out a couple of days back and makes use of images to imagine 'k' states)](https://arxiv.org/pdf/1912.01603.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mAMFd0r-hOS3",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}