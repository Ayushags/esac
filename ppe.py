# -*- coding: utf-8 -*-
"""PPE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PYuzGQUp8AqEiFU1LXXfcMSFidN9u3y0

<h1><center> Continuous Control using Evolution Strategies </center></h1>

Evolution Strategies (ES) have proven to be an effective technique for training continuous as well as discrete control tasks. By making use of Gaussian perterubations in the weight space, ES eliminate the need for backpropagation and reduce the computation time by a significant extent when making use of parallelization. This has allowed scalability in the Reinforcement Learning paradigm.  

This tutorial is a naive implementation of ES proposed in OpenAI's [blog post](https://openai.com/blog/evolution-strategies/) and [paper](https://arxiv.org/pdf/1703.03864.pdf). A detailed implementation of OpenAI's version can be found at their [Github repository](https://github.com/openai/evolution-strategies-starter).  

<h3>NOTE-</h3> All implementations of ES (including this tutorial) require effective computational and parallelization resources such as multiple CPU cores. Experiments for this tutorial were conducted on a virtual AWS EC2 instance consisting of 96 CPU cores and 384 GB of memory. 

<h2>1. Import Dependencies</h2>
We make use of OpenAI's gym environments, namely the classic control and MuJoCo suite. Our model implementation is carried out in PyTorch.
"""

#from google.colab import drive
#user_name = '/content/drive'
#drive.mount(user_name, force_remount=True)
import os
os.environ['OMP_NUM_THREADS'] = '1'
import numpy as np
import sys
#sys.path.append('/content/drive/My Drive/Colab Notebooks/')
#import CyclicMDP
#from CyclicMDP import CyclicMDP
import time, random
import datetime, math, copy
#!pip install tensorboardX
#!pip install --upgrade multiprocessing
#!pip3 install box2d-py==2.3.8
#!pip install roboschool==1.0.48
#!pip install gym==0.15.4 
import tensorboardX
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.multiprocessing as mp
from torch.autograd import Variable
from torch import optim
from torch.distributions import MultivariateNormal
import pickle as pkl

import scipy.stats as ss
from tensorboardX import SummaryWriter
import gym, roboschool
checkpoint_name = './Checkpoint/'
os.sched_setaffinity(os.getpid(), {0})
os.system("taskset -p 0xfffffffffffffff %d" % os.getpid())

"""<h2>2. Network Architecture</h2>
We define a simple architecture consisting of Linear layers and $tanh$ activation. $tanh$ is a suitable choice here as all our action outputs being either torque values or physical force units in gym remain in the [-1,1] range.
"""

class Memory:
    def __init__(self):
        self.actions = []
        self.states = []
        self.logprobs = []
        self.rewards = []
        self.is_terminals = []
    
    def clear_memory(self):
        del self.actions[:]
        del self.states[:]
        del self.logprobs[:]
        del self.rewards[:]
        del self.is_terminals[:]

class NeuralNetwork(nn.Module):
    '''
    Neural network for continuous action space
    '''
    def __init__(self, input_shape, n_actions):
        super(NeuralNetwork, self).__init__()

        self.mlp = nn.Sequential(
            nn.Linear(input_shape, 64),
            nn.Tanh(),
            nn.Linear(64, 64),
            nn.Tanh())

        self.mean_l = nn.Linear(64, n_actions)
        self.mean_l.weight.data.mul_(0.1)

        self.var_l = nn.Linear(64, n_actions)
        self.var_l.weight.data.mul_(0.1)

        self.logstd = nn.Parameter(torch.zeros(n_actions))

    def forward(self, x):
        ot_n = self.mlp(x.float())
        return torch.tanh(self.mean_l(ot_n))



class Critic(nn.Module):
    '''
    Neural network for continuous action space
    '''
    def __init__(self, input_shape, n_actions):
        super(Critic, self).__init__()

        self.mlp = nn.Sequential(
            nn.Linear(input_shape, 64),
            nn.Tanh(),
            nn.Linear(64, 64),
            nn.Tanh())

        self.mean_l = nn.Linear(64, 1)
        self.mean_l.weight.data.mul_(0.1)

        self.var_l = nn.Linear(64, n_actions)
        self.var_l.weight.data.mul_(0.1)

        self.logstd = nn.Parameter(torch.zeros(n_actions))

    def forward(self, x):
        ot_n = self.mlp(x.float())
        return torch.tanh(self.mean_l(ot_n))

"""<h2>3. Evolution Strategies Module<h2/>

ES is genetic algorithm by origin and consists of multiple models. Each model is a part of a braoder set of models called the population. Models in a population are the offsprings from the parent model. Say we have a model (known as the parent model) and we perturb its weights by random Gaussian noise 100 times, we obtain a set of 100 new weights. Thus, we have 100 new models which can be optimized in the weight space. Upon running each of these models once we obtain the episodic reward and gauge their ability to learn. We then create a new model by taking a weighted average over the set of weights of the population. Models with higher returns are given higher preference over weak learners. In various implementations of Evolutionary Computing, only the top $x\%$ models are selected. This is called genetic selection. However, for the purpose of the tutorial we consider all the models in the population. 

The update rule for ES algorithm is mathematically expressed as-  

<center>$\theta_{t+1}\leftarrow \theta_{t} + \alpha\frac{1}{n\sigma}\sum_{i=1}^{n}F_{i}\epsilon_{i}$</center>

Here, $\theta_{t}$ are the parameters at time-step $t$, $\alpha$ the learning rate, $n$ the number of models in population or simply the population size, $\sigma$ the mutation parameter $F_{i}$ the reward corresponding to each model in the population and $\epsilon_{i}$ the Gaussian noise s.t. $\epsilon_{i} \sim \mathcal{N}(0,1)$.
"""

def crossover_inplace(gene1, gene2):
        actor_1 = NeuralNetwork(STATE_DIM, ACTION_DIM)
        actor_1.load_state_dict(gene1)
        actor_2 = NeuralNetwork(STATE_DIM, ACTION_DIM)
        actor_2.load_state_dict(gene2)
        # print(actor_1.state_dict())
        for param1, param2 in zip(actor_1.parameters(), actor_2.parameters()):

            # References to the variable tensors
            W1 = param1.data
            W2 = param2.data

            if len(W1.shape) == 2: #Weights no bias
                num_variables = W1.shape[0]
                # Crossover opertation [Indexed by row]
                num_cross_overs = random.randrange(num_variables*2)  # Lower bounded on full swaps
                for i in range(num_cross_overs):
                    receiver_choice = random.random()  # Choose which gene to receive the perturbation
                    if receiver_choice < 0.5:
                        ind_cr = random.randrange(W1.shape[0])  #
                        W1[ind_cr, :] = W2[ind_cr, :]
                    else:
                        ind_cr = random.randrange(W1.shape[0])  #
                        W2[ind_cr, :] = W1[ind_cr, :]

            elif len(W1.shape) == 1: #Bias
                num_variables = W1.shape[0]
                # Crossover opertation [Indexed by row]
                num_cross_overs = random.randrange(num_variables)  # Lower bounded on full swaps
                for i in range(num_cross_overs):
                    receiver_choice = random.random()  # Choose which gene to receive the perturbation
                    if receiver_choice < 0.5:
                        ind_cr = random.randrange(W1.shape[0])  #
                        W1[ind_cr] = W2[ind_cr]
                    else:
                        ind_cr = random.randrange(W1.shape[0])  #
                        W2[ind_cr] = W1[ind_cr]


def sample_noise(neural_net):
    '''
    Sample noise for each parameter of the neural net
    '''
    nn_noise = []
    for n in neural_net.parameters():
        noise = np.random.normal(size=n.data.numpy().shape)
        nn_noise.append(noise)
    return np.array(nn_noise)

def evaluate_neuralnet(nn, env):
    '''
    Evaluate an agent running it in the environment and computing the total reward
    '''
    obs = env.reset()
    critic = Critic(STATE_DIM, ACTION_DIM)
    game_reward = 0
    reward = 0
    done = False
    tot_adv = 0

    while True:
        # Output of the neural net
        obs = torch.tensor(obs)
        net_output = nn(obs)
        state_val = critic(obs)
        cov_mat = torch.diag(action_var)
        dist = MultivariateNormal(net_output, cov_mat)
        action = dist.sample()
        action_logprob = dist.log_prob(action)
        # the action is the value clipped returned by the nn
        # action = net_output.data.numpy().argmax()
        action = np.clip(action.data.cpu().numpy().squeeze(), -1, 1)
        # action = action.data.numpy().argmax()
        # action = np.asarray([action])
        new_obs, reward, done, _ = env.step(action)
        
        obs = new_obs

        advantages = (reward - state_val)
        tot_adv += advantages
        game_reward += reward

        if done:
            break
        
    return game_reward, tot_adv.detach()

def evaluate_noisy_net(noise, neural_net, env, elite_queue):
    '''
    Evaluate a noisy agent by adding the noise to the plain agent
    '''
    old_dict = neural_net.state_dict()

    # add the noise to each parameter of the NN
    for n, p in zip(noise, neural_net.parameters()):
        p.data += torch.FloatTensor(n * STD_NOISE)

    elite_queue.put(neural_net.state_dict())

    # evaluate the agent with the noise
    reward, adv = evaluate_neuralnet(neural_net, env)
    # load the previous paramater (the ones without the noise)
    neural_net.load_state_dict(old_dict)
    
    return reward, adv

def worker(params_queue, output_queue, elite_queue):
    '''
    Function execute by each worker: get the agent' NN, sample noise and evaluate the agent adding the noise. Then return the seed and the rewards to the central unit
    '''

    env = gym.make(ENV_NAME)
    # env = CyclicMDP()
    actor = NeuralNetwork(STATE_DIM, ACTION_DIM)
    # actor = NeuralNetwork(3,3)
    while True:
        # get the new actor's params
        act_params = params_queue.get()
        if act_params != None:
            # load the actor params
            actor.load_state_dict(act_params)

            # get a random seed
            seed = np.random.randint(1e6)
            # set the new seed
            np.random.seed(seed)

            noise = sample_noise(actor)

            pos_rew, pos_adv = evaluate_noisy_net(noise, actor, env, elite_queue)
            # Mirrored sampling
            neg_rew, neg_adv = evaluate_noisy_net(-noise, actor, env, elite_queue)

            output_queue.put([[pos_rew, neg_rew], [pos_adv, neg_adv], seed])
        else:
            break


def check_ppo(prev_reward,current_reward,num_models):
    if num_models < 2:
        num_models = 2
    else:
        if prev_reward > current_reward:
            num_models -= 1
        else:
            num_models += 1
    return num_models


def normalized_rank(rewards):
    '''
    Rank the rewards and normalize them.
    '''
    ranked = ss.rankdata(rewards)
    norm = (ranked - 1) / (len(ranked) - 1)
    norm -= 0.5
    return norm

"""<h2>4. Training Loop</h2>

We execute the typical Reinforcement Learning training loop for a finite number of episodes. The parameter $MAX\_WORKERS$ should be handled carefully since training on multiple CPUs requires more processes and can often lead to poor performance. $\sigma$ is a tricky hyperparameter to tune. Often at times it will yield good evolutions in weight spaces but for some environments it may not lead to full convergence.
"""

rolling_rewards = []
max_rewards = []
min_rewards = []
avg_rewards = [0]
time_list = []
elite_learners = []
elite_array = []
elite_list = []

ENV_NAME = 'RoboschoolHopper-v1'
env = gym.make(ENV_NAME)
memory = Memory()
# env = CyclicMDP()

# Hyperparameters
ES_INTERVAL = 1
UPD_INTERVAL = 5
ELITISM_RATE = 0.2
LAMBDA = 0.005
EPOCHS = 80
GAMMA = 0.99
EPS_CLIP = 0.2
STD_NOISE = 0.01
BATCH_SIZE = 50
NUM_PPO_MODELS = 20
NUM_WINNERS = int(ELITISM_RATE*BATCH_SIZE)
LEARNING_RATE = 0.01
LEARNING_RATE_PPO = 0.0001
MAX_ITERATIONS = 3000
ACTION_STD = 0.5
STATE_DIM = env.observation_space.shape[0]
ACTION_DIM = env.action_space.shape[0]
action_var = torch.full((ACTION_DIM,), ACTION_STD*ACTION_STD)
loss = torch.Tensor([1])

MAX_WORKERS = 30

save_video_test = False
VIDEOS_INTERVAL = 100

now = datetime.datetime.now()
date_time = "{}_{}.{}.{}".format(now.day, now.hour, now.minute, now.second)

epsilon_start = 0.2
epsilon_final = 0.001
epsilon_decay = 1000

epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)

if __name__ == '__main__':
    # Writer name
    writer_name = 'ASY_ES_{}_{}_{}_{}_{}_{}'.format(ENV_NAME, date_time, str(STD_NOISE), str(BATCH_SIZE), str(LEARNING_RATE), str(MAX_ITERATIONS), str(MAX_WORKERS))
    print('Name:', writer_name)

    # Create the test environment
    if save_video_test:
        env = gym.wrappers.Monitor(env,  "VIDEOS/TEST_VIDEOS_"+writer_name, video_callable=lambda episode_id: True)

    # Initialize the agent
    actor = NeuralNetwork(STATE_DIM, ACTION_DIM)
    critic = Critic(STATE_DIM, ACTION_DIM)
    actor_ppo = NeuralNetwork(STATE_DIM, ACTION_DIM)
    # actor = NeuralNetwork(3,3)

    # Initialize the optimizer
    optimizer = optim.Adam(actor.parameters(), lr=LEARNING_RATE)
    optimizer_ppo = optim.Adam(actor_ppo.parameters(), lr=LEARNING_RATE_PPO)
    # torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.999)
    MseLoss = nn.MSELoss()

    writer = SummaryWriter(log_dir='content/runs/'+writer_name)

    # Queues to pass and get the variables to and from each processe
    output_queue = mp.Queue(maxsize=BATCH_SIZE)
    params_queue = mp.Queue(maxsize=BATCH_SIZE)
    elite_queue = mp.Queue(maxsize=int(2*BATCH_SIZE))

    processes = []

    # # create the queue with the actor parameters
    # for _ in range(BATCH_SIZE):
    #     params_queue.put(actor.state_dict())
    
    # Create and start the processes
    for _ in range(MAX_WORKERS):
        p = mp.Process(target=worker, args=(params_queue, output_queue, elite_queue))
        p.start()
        processes.append(p)

    for _ in range(0,NUM_WINNERS):
        elite_list.append(actor.state_dict())


    # Execute the main loop MAX_ITERATIONS times
    for n_iter in range(1,MAX_ITERATIONS+1):
        it_time = time.time()

        batch_noise = []
        batch_reward = []
        batch_loss = []
        batch_ratio = []
        batch_adv = []

        # Winner crossovers shielded from noise
        for h in range(0,int(NUM_WINNERS)):
          dict_copy = copy.deepcopy(actor.state_dict())
          crossover_inplace(elite_list[h],dict_copy) #elite_list[h]
          # params_queue.put(elite_list[h])
          params_queue.put(dict_copy)
          # params_queue.put(actor.state_dict())

        # Add noise to parameters
        for h in range(NUM_WINNERS,BATCH_SIZE-NUM_PPO_MODELS):
          params_queue.put(actor.state_dict())
        
        # Gradient-based parameters
        for _ in range(NUM_PPO_MODELS):
          dict_copy = copy.deepcopy(actor.state_dict())
          ppo_copy = copy.deepcopy(actor_ppo.state_dict())
          crossover_inplace(ppo_copy,dict_copy)
          params_queue.put(dict_copy)

        ############################### PPO UPDATE ##################################

        if n_iter % UPD_INTERVAL == 0:
            
          for _ in range(10):
              done = False
              state = env.reset()
              while done==False:
                # Running policy_old:
                action_mean = actor_ppo(torch.Tensor(state))
                cov_mat = torch.diag(action_var)     
                dist = MultivariateNormal(action_mean, cov_mat)
                action = dist.sample()
                action_logprob = dist.log_prob(action)
                # action = np.argmax(action)
                action = np.clip(action.data.cpu().numpy().squeeze(), -1, 1)
                # action = np.asarray([action])
                next_state, reward, done, _ = env.step(action)

                # Saving reward and is_terminals:
                memory.states.append(torch.Tensor(state))
                memory.actions.append(torch.Tensor(action))
                memory.logprobs.append(torch.Tensor([action_logprob]))
                memory.rewards.append(reward)
                memory.is_terminals.append(done)

          
          old_states = torch.squeeze(torch.stack(memory.states), 1).detach()
          # old_actions = torch.squeeze(torch.stack(memory.actions), 1).detach()
          old_actions = torch.stack(memory.actions).detach()
          old_logprobs = torch.squeeze(torch.stack(memory.logprobs), 1).detach()
          
          rewards = []
          discounted_reward = 0
          for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):
            if is_terminal:
                discounted_reward = 0
            discounted_reward = reward + (GAMMA * discounted_reward)
            rewards.insert(0, discounted_reward)
          rewards = torch.Tensor(rewards)
          

          # Optimize policy for K epochs:
          for _ in range(EPOCHS):
            # Evaluating old actions and values :
            action_mean = actor_ppo(old_states)
            
            action_var_exp = action_var.expand_as(action_mean)
            cov_mat = torch.diag_embed(action_var_exp)
            
            dist = MultivariateNormal(action_mean, cov_mat)
            
            action_logprobs = dist.log_prob(old_actions)
            dist_entropy = dist.entropy()
            state_values = critic(old_states)
            
            # Finding the ratio (pi_theta / pi_theta__old):
            ratios = torch.exp(action_logprobs - old_logprobs.detach())

            # Finding Surrogate Loss:
            advantages = rewards - state_values.detach()
            surr1 = ratios * advantages
            surr2 = torch.clamp(ratios, 1-EPS_CLIP, 1+EPS_CLIP) * advantages
            loss = -torch.min(surr1, surr2) + 0.5*MseLoss(state_values, rewards) - 0.01*dist_entropy
            
            # take gradient step
            optimizer_ppo.zero_grad()
            loss.mean().backward()
            optimizer_ppo.step()
            
          memory.clear_memory()    
        #------------------------------------------------------------------------------#

        # receive from each worker the results (the seed and the rewards)
        
        for i in range(BATCH_SIZE):
            p_rews, p_adv, p_seed = output_queue.get()
            
            np.random.seed(p_seed)
            noise = sample_noise(actor)
            batch_noise.append(noise)
            batch_noise.append(-noise)

            batch_reward.append(p_rews[0]) # reward of the positive noise
            batch_reward.append(p_rews[1]) # reward of the negative noise

            batch_adv.append(p_adv[0]) # adv for positive noise
            batch_adv.append(p_adv[1]) # adv for negative noise

        batch_adv = torch.FloatTensor(batch_adv)
        
        elite_array = [] # only a placeholder for getting parameters from queue
        elite_list = [] # stores weights of winners
        for _ in range(int(2*BATCH_SIZE)):
          elite_array.append(elite_queue.get())
        elite_list = [x for _,x in sorted(zip(batch_reward,elite_array),reverse=True)]
        
        
        ############################# JUST PRINTING DATA ##############################
        avg_reward = np.round(np.mean(batch_reward), 2)
        print(n_iter, 'Mean:',np.round(avg_reward,2), 'Max:', np.round(np.max(batch_reward), 2), 'Time:', np.round(time.time()-it_time, 2),'PPO Models:',NUM_PPO_MODELS)
        writer.add_scalar('reward', np.mean(batch_reward), n_iter)
        if len(avg_rewards)>10:
          rolling_rewards.append(np.mean(avg_rewards[-10:]))
        max_rewards.append(np.max(batch_reward))
        min_rewards.append(np.min(batch_reward))
        # NUM_PPO_MODELS = check_ppo(avg_rewards[-1],avg_reward,NUM_PPO_MODELS)
        avg_rewards.append(avg_reward)
        time_list.append(time.time()-it_time)
        count = 0
        for k in batch_reward:
          if k > avg_reward:
            count += 1
        elite_learners.append(count)
            
        #------------------------------------------------------------------------------#


        ################################## ES UPDATE ##################################
        
        # Rank the reward and normalize it
        if n_iter % ES_INTERVAL == 0:
            batch_reward = torch.FloatTensor(normalized_rank(batch_reward))
            weight_comb = epsilon_by_frame(n_iter)
            
            th_update = []
            optimizer.zero_grad()
            # for each actor's parameter, and for each noise in the batch, update it by the reward * the noise value
            for idx, p in enumerate(actor.parameters()):
                upd_weights = torch.zeros(p.data.shape)

                for n,r in zip(batch_noise, batch_reward):
                    upd_weights += (r) * torch.Tensor(n[idx])#weight_comb*r+ (1-weight_comb)*loss.mean() 

                upd_weights = upd_weights / (BATCH_SIZE*STD_NOISE)
                # put the updated weight on the gradient variable so that afterwards the optimizer will use it
                p.grad = torch.FloatTensor( -upd_weights).clone()

            # Optimize the actor's NN
            optimizer.step()

        if n_iter % VIDEOS_INTERVAL == 0:
            # print('Test reward:',evaluate_neuralnet(actor, env))
            torch.save({'model_state_dict': actor.state_dict(), 'optimizer_state_dict': optimizer.state_dict()},
                      checkpoint_name+'/actor.pth.tar')
        
        #-----------------------------------------------------------------------------#
                    
    # quit the processes
    for _ in range(MAX_WORKERS):
        params_queue.put(None)

    for p in processes:
        p.join()

    data_save = {}
    data_save['elite_learners'] = elite_learners
    data_save['time'] = time_list
    data_save['rolling_rewards'] = rolling_rewards
    data_save['max_rewards'] = max_rewards
    data_save['min_reward'] = min_rewards
    data_save['avg_reward'] = avg_rewards

    with open(checkpoint_name+'data.pkl', 'wb') as f:
        pkl.dump(data_save, f)



"""<h2>5. Visualize Results</h2>

As always, it is good practice to view the performance of our algorithm. In the case of ES we visualize the episodic and average rewards. We also gain intuition of the overall performance of our population by taking into the number of elite learners, which are defined as the number of models above the average score. As the population converges to a global minima, the number of elite learners increase to a constant indicating the successful convergence of the overall population.  

Here are some of the tasks and the performance of our agents-  

<img src="gifs/Hopper.gif" align="left" height="200" width="200" /><img src="gifs/Swimmer.gif" align="left" height="200" width="200" /><img src="gifs/HalfCheetah.gif" align="left" height="200" width="200" /><img src="gifs/InvertedPendulum.gif" align="left" height="200" width="200" />
<img src="gifs/Reacher.gif" align="left" height="200" width="200" /><img src="gifs/Humanoid.gif" align="left" height="200" width="200" /><img src="gifs/BipedalWalker.gif" align="left" height="200" width="200" /><img src="gifs/LunarLander.gif" align="left" height="200" width="200" />
"""

#import matplotlib.pyplot as plt
#import seaborn as sns
#sns.set_style('dark', {'axes.grid' : True})

#plt.figure(figsize=(20,6))

#plt.subplot(141)
#plt.title('Episodic Reward')
#plt.plot(np.arange(0,len(avg_rewards),1),avg_rewards, color='royalblue')
#plt.fill_between(np.arange(0,len(avg_rewards),1),min_rewards,max_rewards,facecolor='lightsteelblue',linewidth=0)


#plt.subplot(142)
#plt.title('Rolling Average Reward')
#plt.plot(rolling_rewards)

#plt.subplot(143)
#plt.title('Time Per Episode')
#plt.plot(time_list)

#plt.subplot(144)
#plt.title('Elite Learners')
#plt.plot(elite_learners)

#env_list = ['Acrobot-v1','BipedalWalker-v2','CartPole-v1','LunarLanderContinuous-v2','Hopper-v2','Swimmer-v2','HalfCheetah-v2',
#            'InvertedPendulum-v2','Reacher-v2','Humanoid-v2','HumanoidStandup-v2']
#import matplotlib.pyplot as plt
#import seaborn as sns

#sns.set_style('dark', {'axes.grid' : True})

#for j in env_list:
#  check_name = checkpoint_name+j+'/data.pkl'
#  data = pkl.load(open(check_name,'rb'))
#  if j=='HalfCheetah-v2':
#    x = 300
#  else:
#    x = -1
#  avg_rewards = data['avg_reward'][:x]
#  max_rewards = data['max_rewards'][:x]
#  min_rewards = data['min_reward'][:x]
#  rolling_rewards = data['rolling_rewards'][:x]
#  time_list = data['time'][:x]
#  elite_learners = data['elite_learners'][:x]

#  plt.figure(figsize=(20,6))
#  plt.suptitle(j,fontsize='16')

#  plt.subplot(141)
#  plt.title('Episodic Reward')
#  plt.plot(np.arange(0,len(avg_rewards),1),avg_rewards, color='royalblue')
#  plt.fill_between(np.arange(0,len(avg_rewards),1),min_rewards,max_rewards,facecolor='lightsteelblue',linewidth=0)

#  plt.subplot(142)
#  plt.title('Rolling Average Reward')
#  plt.plot(rolling_rewards)

#  plt.subplot(143)
#  plt.title('Time Per Episode')
#  plt.plot(time_list)

#  plt.subplot(144)
#  plt.title('Elite Learners')
#  plt.plot(elite_learners)

"""<h2>6. References</h2>

1. [OpenAI Blog](https://openai.com/blog/evolution-strategies/)
2. [Paper](https://arxiv.org/pdf/1703.03864.pdf)
3. [Github Repository](https://github.com/openai/evolution-strategies-starter)
"""
